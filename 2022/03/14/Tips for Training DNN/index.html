<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|EB Garamond:300,300italic,400,400italic,700,700italic|Source Code Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"aeowind.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="切断电缆 朝霞晚风 临时收入 临时生活">
<meta property="og:type" content="article">
<meta property="og:title" content="Tips for Training DNN">
<meta property="og:url" content="https://aeowind.github.io/2022/03/14/Tips%20for%20Training%20DNN/index.html">
<meta property="og:site_name" content="Aeo&#39;s Blog">
<meta property="og:description" content="切断电缆 朝霞晚风 临时收入 临时生活">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-03-14T13:50:55.516Z">
<meta property="article:modified_time" content="2022-03-14T14:29:33.081Z">
<meta property="article:author" content="aeowind">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://aeowind.github.io/2022/03/14/Tips%20for%20Training%20DNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tips for Training DNN | Aeo's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Aeo's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你要静候 再静候</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-fw fa-film"></i>观影</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-fw fa-book"></i>阅读</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://aeowind.github.io/2022/03/14/Tips%20for%20Training%20DNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="aeowind">
      <meta itemprop="description" content="爱上一场认真的消遣">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aeo's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tips for Training DNN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-14 21:50:55 / 修改时间：22:29:33" itemprop="dateCreated datePublished" datetime="2022-03-14T21:50:55+08:00">2022-03-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description"><blockquote class="blockquote-center">切断电缆 朝霞晚风 临时收入 临时生活</blockquote></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>本文的主要思路：针对training set和testing
set上的performance分别提出针对性的解决方法 1、在training
set上准确率不高： new activation function：ReLU、Maxout adaptive
learning rate：Adagrad、RMSProp、Momentum、Adam 2、在testing
set上准确率不高：Early Stopping、Regularization or Dropout</p>
</blockquote>
<h3 id="recipe">Recipe</h3>
<h4 id="three-steps-of-deep-learning">three steps of deep learning</h4>
<p>做深度学习也遵循这三个步骤：</p>
<ul>
<li>define a set of function，</li>
<li>goodness of function，找到loss function</li>
<li>pick the best function，找到使loss最小化的参数</li>
</ul>
<p>overfitting是指模型在训练集上表现良好，但在测试集上表现却很差的现象。</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607122016217.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607122016217.png"
alt="image-20200607122016217" /></a></p>
<h4 id="do-not-always-blame-overfitting">Do not always blame
Overfitting</h4>
<p>在下图中，我们展示了一个20层和56层的network，在训练集和测试集上的error。黄色表示20层network，红色表示56层network。</p>
<p>由于模型在训练集上的表现，20层的network表现得比较好，有同学就认为这是overfitting，但其实这并不是overfitting问题，因为这个模型在训练集上的表现，也是20层的network表现好</p>
<p>之所以出现这个20层和56层network表现（都是20层network表现好），是由于模型的训练没有训练好</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123330505.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123330505.png"
alt="image-20200607123330505" /></a></p>
<h4 id="different-approaches-for-different-problems.">Different
approaches for different problems.</h4>
<p>在网络的训练过程中，我们要针对网络的不同问题提供不同的解决方法，主要有两个问题</p>
<ul>
<li>在training data上表现不好</li>
<li>在testing data上表现不好</li>
</ul>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123926844.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607123926844.png"
alt="image-20200607123926844" /></a></p>
<h3 id="good-results-on-training-data">Good Results on Training
Data?</h3>
<p>要在training data上获得好的结果，可以使用new activation
function和adaptive learning rate</p>
<h4 id="new-activation-function">New Activation Function</h4>
<h5 id="deeper-is-better">deeper is better ？</h5>
<p>在1980年代，network中主要使用sigmoid
funcion作为激活函数，从下图中我们可以看出，使用sigmoid
function并不能保证网络结构越深，训练结果越好</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607124417000.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607124417000.png"
alt="image-20200607124417000" /></a></p>
<h5 id="vanishing-gradient-problem">Vanishing Gradient Problem</h5>
<p>出现上面这个问题的原因并不是overfitting，而是vanishing
gradient（梯度消失）</p>
<p>当网络层数很深的时候，在靠近input layer的位置，常常会有很小的
gradient，学习速度也很慢；而在靠近output
layer的地方，常常会有更大的gradient，学习速度也会很快，很快就到了converge（收敛）了</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125209255.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125209255.png"
alt="image-20200607125209255" /></a></p>
<p>下面将叙述出现这个问题的原因。对于下图的中sigmoid
function，输出范围为[0,1]，对于很大的输入，输出往往会被压缩成一个较小的值</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125550128.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125550128.png"
alt="image-20200607125550128" /></a></p>
<p>对于loss
function对其中一个参数w的微分∂l/∂w，这里我们将其表达式写为Δw，可以表示当前参数w对结果loss的影响，当把这个参数w进行变化时，对loss的影响会有多大</p>
<p>如下图所示，如果我们输入一个很大的Δw，在经过sigmoid
function运算之后，其值就缩小一次；当经过后面多层的压缩之后，Δw的值就变得越来越小;……；因此gradient在input
layer附近的值会很大，但在output
layer附近的值经过多次的压缩就变得很小了。</p>
<p>当缩小的gradient达到我们设置的那个临界值，即gradient接近于0，就发生了梯度消失问题</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125910525.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607125910525.png"
alt="image-20200607125910525" /></a></p>
<p>要解决这个问题，可以修改一下network中用到的激活函数</p>
<h5 id="relu">ReLU</h5>
<p>我们选取ReLU函数的原因有以下几个：</p>
<ul>
<li>可以快速计算，不管是函数值还是对应的梯度</li>
<li>结合了生物上的一些观察</li>
<li>无穷多个不同bias的sigmoid function叠加的结果可以变成ReLU</li>
<li>可以解决梯度消失问题；</li>
</ul>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131402741.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131402741.png"
alt="image-20200607131402741" /></a></p>
<p>使用ReLU函数之后，代入具体的网络结构</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131816149.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131816149.png"
alt="image-20200607131816149" /></a></p>
<p>对于input为0的值，network将不再计算其相对应的weight，而对于input不为0的值，就相当于一个线性函数y=x。这样做可以简化网络结构，网络结构中也就不存在gradient很小的neural</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131920750.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607131920750.png"
alt="image-20200607131920750" /></a></p>
<p>Q：但这时出现了一个新问题，ReLU函数在z=0这一点是不可导的，那么我们在根据loss
function如何来计算gradient呢？</p>
<p>A：这里我们将输入z&lt;0的数的gradient看作0，相当于从network中抹去了这部分神经元；对于z&gt;=0的数，gradient=1</p>
<h5 id="relu---variant">ReLU - variant</h5>
<p>对于ReLU，当x&lt;=0时，函数的输出值就为0了，网络中的参数也没办法更新。因此，就有学者提出了Leaky
ReLU，当x&lt;=0时，函数的输出值不是0，而是乘以一个系数0.01，这时的函数就称作<strong>Leaky
ReLU</strong></p>
<p>还有另外一种ReLU函数的变体，<strong>Parametric
ReLU</strong>，前面乘上的系数也可以进行训练</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607143129117.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607143129117.png"
alt="image-20200607143129117" /></a></p>
<h5 id="maxout">Maxout</h5>
<blockquote>
<p>ReLU is a special cases of Maxout</p>
</blockquote>
<p>Maxout的主要思想是：让network自己去学习对应的activation
function，可以学习出ReLU，也可以是其他的activation function</p>
<p>Maxout激活函数是对前几个神经元取最大值，再输出相应的最大值</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144717796.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144717796.png"
alt="image-20200607144717796" /></a></p>
<p><strong>Maxout-&gt;ReLU</strong></p>
<p>在下图中，对于左图中的ReLU function</p>
<ul>
<li><p>input为蓝色直线，表示z=wx+b，</p></li>
<li><p>output：当z&lt;0时，relu也输出为0；当z&gt;0时，ReLU的图像和z是一致的</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144943655.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607144943655.png"
alt="image-20200607144943655" /></a></p></li>
</ul>
<p>对于右图中的Maxout
function，z1对应的权重是w,b，而z2z2对应的权重则是0，</p>
<ul>
<li>input为z1,z2，
<ul>
<li>蓝色直线表示z1=wx+b，</li>
<li>红色表示z2=0</li>
</ul></li>
<li>这时neural的output为max{z1,z2}，输出则是和ReLU一致的（图中绿色直线）</li>
</ul>
<p><strong>Maxout-&gt;more than ReLU</strong></p>
<p>maxout不仅可以学习ReLU，也可以学习其他的activation function</p>
<p>对于右图中的新的输入，z2对应的权重则变成了w′,b′，那么相应的input和output为</p>
<ul>
<li>input，z1=wx+b,z2=w′x+b′，分别对应图中蓝色和绿色直线；</li>
<li>output为max{z1,z2}，表现为图中绿色直线</li>
</ul>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607150829520.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607150829520.png"
alt="image-20200607150829520" /></a></p>
<p>这时我们得到的activation
function就是图中的绿色直线，是通过网络training出来的，训练的参数为w,w′,b,b′，训练结束即可得出我们的activation
function</p>
<p><strong>Summary</strong></p>
<p>这里我们先对maxout做一个总结，maxout是一个可学习的activation
function，可以学习出任何分段线性凸函数（piecewise linear convex
function），具体的分段数取决于在group中的元素个数</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607151653515.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607151653515.png"
alt="image-20200607151653515" /></a></p>
<p>对于maxout函数的训练，如果是下图中的网络结构，我们假设已经知道z11,z14,z22,z23为对应的最大值，</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152054887.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152054887.png"
alt="image-20200607152054887" /></a></p>
<p>那么network可以再次被化简，neural也可以变少</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152242384.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152242384.png"
alt="image-20200607152242384" /></a></p>
<h4 id="adaptive-learning-rate">Adaptive Learning Rate</h4>
<h5 id="review">Review</h5>
<p><strong>Adagrad</strong></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152520774.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607152520774.png"
alt="image-20200607152520774" /></a></p>
<p><strong>RMSprop</strong></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153455066.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153455066.png"
alt="image-20200607153455066" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153507651.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153507651.png"
alt="image-20200607153507651" /></a></p>
<p>gradient为0的点可以是local minimum，也可以是saddle
point，还可以是在很平缓的plateau中的某些点</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153541752.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153541752.png"
alt="image-20200607153541752" /></a></p>
<p>而在物理世界，物体本身是带有momentum的，再加上gradient的作用，就很可能可以跳出saddle
point，继续训练</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153719322.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607153719322.png"
alt="image-20200607153719322" /></a></p>
<h5 id="momentum">Momentum</h5>
<p>图中蓝色箭头表示Movement（前进方向），红色箭头表示gradient的方向，绿色虚线表示上一次movement对本次的影响（惯性）</p>
<p>再θ0θ0处，movement为v0=0；对于在θ1处的前进方向，先计算出在θ0处的梯度ΔL(θ0)，我们要移动的方向是由上一个时间点的gradient为ΔL(θ0)和前进方向v0决定的，即</p>
<p>v1=λv0−ηΔL(θ0)=−ηΔL(θ0)</p>
<p>其中λ也是一个可以手动调整的参数</p>
<p>对于下一个时间点的移动方向v2，是和当前时间节点的移动方向和梯度v1,ΔL(θ1)决定的，即</p>
<p>v2=λv1−ηΔL(θ1)=−ληΔL(θ0)−ηΔL(θ1)</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607154125395.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607154125395.png"
alt="image-20200607154125395" /></a></p>
<p>再回到之前的例子，红色箭头表示gradient的反方向，绿色表示momentum的方向，蓝色箭头表示受到gradient和momentum影响后的真实运动方向</p>
<p>初始点的momentum值为0；在下一个plateau上的点，虽然gradient的值很小很小，但由于受到上一个很大的momentum的影响，真实的movement还是向前的，步长也没有因为gradient的变小而变得很小；</p>
<p>如果我们现在走到了local
minimum，此时gradient=0，此时由于momentum的影响，如果momentum的值足够大，还会继续向前运动</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607161327324.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607161327324.png"
alt="image-20200607161327324" /></a></p>
<h5 id="adam">Adam</h5>
<blockquote>
<p>RMSProp + Momentum</p>
<p>Adam其实就是结合了RMSProp和Momentum思想的方法</p>
</blockquote>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607162646245.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607162646245.png"
alt="image-20200607162646245" /></a></p>
<p>先将动量momentum和初始移动方向初始化为0，即m0=0,v0=0，v0表示RMSProp中分母上的参数σ</p>
<p>计算在t时的梯度gt，</p>
<p>gt=Δθft(θθ−1)</p>
<p>根据上一个时间点的要走的方向mtmt和gradient，因此t时的移动方向为mtmt———Momentum</p>
<p>mt=β1⋅mt−1+(1−β1)⋅gt</p>
<p>根据上一个时间点的移动方向vt−1和gradient，则此时的真实移动方向vtvt为———-RMSprop</p>
<p>vt=β1⋅vt−1+(1−β2)⋅gt</p>
<p>该算法还进行了bias corrected，</p>
<p>m<sup>t=mt(1−βt1),v</sup>t=vt(1−βt2)</p>
<p>将进行了bias corrected的参数再输入公式，更新参数</p>
<p>θt=θt−1−α⋅m<sup>t/v</sup>t−−√+ϵ</p>
<h3 id="good-results-on-testing-data">Good Results on Testing Data?</h3>
<h4 id="early-stopping">Early Stopping</h4>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163018382.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163018382.png"
alt="image-20200607163018382" /></a></p>
<h4 id="regularization">Regularization</h4>
<h5 id="l2-regularization">L2 Regularization</h5>
<p>正则化就引入了一个新的loss
function，加上了一个新的正则项，这个正则项将所有需要training的参数都包括进来了，通常不包括bias</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163135769.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163135769.png"
alt="image-20200607163135769" /></a></p>
<p>这个新的loss function再对w求偏微分，对参数进行更新</p>
<p>wt+1=(1−ηλ)wt−η∂L∂w</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163452933.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607163452933.png"
alt="image-20200607163452933" /></a></p>
<p>与原来的参数更新公式相比，可以发现wtwt前面多了一项(1−ηλ)，通常这个η,λ都是很小的值，这里我们假设(1−ηλ)是很接近于1的值，约等于0.99;
regularization所做的事就是，在每次更新参数时，全都在前面乘上了一个小于1的数，在经过若干次的训练之后，(1−ηλ)wt的值就很接近0了</p>
<p>虽然(1−ηλ)wt的值每次都会变得越来越小，但参数更新的公式中，后面还有另外一项η∂L∂w，会使得梯度的值不会变成0，达到平衡</p>
<h5 id="l1-regularization">L1 Regularization</h5>
<p>既然L2可以作为正则项，L1也可以作为正则项，正则项为参数的绝对值相加。对这些参数求导，当wi大于0时，gradient=1，当wi小雨0时，gradient=-1，即为sgn(w)函数</p>
<p>L1的参数更新公式为</p>
<p>wt+1=wt−η∂L∂w−ηλ sgn(wt)</p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607171442670.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607171442670.png"
alt="image-20200607171442670" /></a></p>
<p>与原来的参数更新公式相比较，可以发现后面多了一项−ηλ
sgn(wt)，表示参数在原来的基础上都要减去一个小于1的数</p>
<h5 id="l1-vs-l2">L1 vs L2</h5>
<p>参数更新公式分别如下，</p>
<p>L1: wt+1=wt−η∂L∂w−ηλ sgn(wt)L2: wt+1=(1−ηλ)wt−η∂L∂w</p>
<ul>
<li>L1参数更新公式每次都多<strong>减去了一个小于1的固定值（constant）</strong></li>
<li>L2参数更新公式中每次都将<strong>前一次的参数乘上一个小于1的值</strong></li>
</ul>
<h4 id="dropout">Dropout</h4>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173403475.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173403475.png"
alt="image-20200607173403475" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173419426.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173419426.png"
alt="image-20200607173419426" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173455150.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173455150.png"
alt="image-20200607173455150" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173629804.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173629804.png"
alt="image-20200607173629804" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173828093.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173828093.png"
alt="image-20200607173828093" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173729715.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173729715.png"
alt="image-20200607173729715" /></a></p>
<p><a
target="_blank" rel="noopener" href="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173943611.png"><img
src="https://gitee.com/scarleatt/image/raw/master/img/image-20200607173943611.png"
alt="image-20200607173943611" /></a></p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 学习</a>
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"><i class="fa fa-tag"></i> 算法</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/12/Backpropagation/" rel="prev" title="backpropagation">
      <i class="fa fa-chevron-left"></i> backpropagation
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#recipe"><span class="nav-number">1.</span> <span class="nav-text">Recipe</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#three-steps-of-deep-learning"><span class="nav-number">1.1.</span> <span class="nav-text">three steps of deep learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#do-not-always-blame-overfitting"><span class="nav-number">1.2.</span> <span class="nav-text">Do not always blame
Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#different-approaches-for-different-problems."><span class="nav-number">1.3.</span> <span class="nav-text">Different
approaches for different problems.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#good-results-on-training-data"><span class="nav-number">2.</span> <span class="nav-text">Good Results on Training
Data?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#new-activation-function"><span class="nav-number">2.1.</span> <span class="nav-text">New Activation Function</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#deeper-is-better"><span class="nav-number">2.1.1.</span> <span class="nav-text">deeper is better ？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#vanishing-gradient-problem"><span class="nav-number">2.1.2.</span> <span class="nav-text">Vanishing Gradient Problem</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#relu"><span class="nav-number">2.1.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#relu---variant"><span class="nav-number">2.1.4.</span> <span class="nav-text">ReLU - variant</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#maxout"><span class="nav-number">2.1.5.</span> <span class="nav-text">Maxout</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adaptive-learning-rate"><span class="nav-number">2.2.</span> <span class="nav-text">Adaptive Learning Rate</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#review"><span class="nav-number">2.2.1.</span> <span class="nav-text">Review</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#momentum"><span class="nav-number">2.2.2.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#adam"><span class="nav-number">2.2.3.</span> <span class="nav-text">Adam</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#good-results-on-testing-data"><span class="nav-number">3.</span> <span class="nav-text">Good Results on Testing Data?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#early-stopping"><span class="nav-number">3.1.</span> <span class="nav-text">Early Stopping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#regularization"><span class="nav-number">3.2.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#l2-regularization"><span class="nav-number">3.2.1.</span> <span class="nav-text">L2 Regularization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#l1-regularization"><span class="nav-number">3.2.2.</span> <span class="nav-text">L1 Regularization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#l1-vs-l2"><span class="nav-number">3.2.3.</span> <span class="nav-text">L1 vs L2</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout"><span class="nav-number">3.3.</span> <span class="nav-text">Dropout</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="aeowind"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">aeowind</p>
  <div class="site-description" itemprop="description">爱上一场认真的消遣</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/aeowind" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;aeowind" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/129971630/" title="douban → https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;129971630&#x2F;" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>douban</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aeowind</span>
</div>



  <script>
    var OriginTitle = document.title;
    var titleTime;
    document.addEventListener('visibilitychange', function() {
      if (document.hidden) {
        document.title = '(*^▽^*)我藏好了哦~' + OriginTitle;
        clearTimeout(titleTime);
      } else {
        document.title = 'q(≧▽≦q)被你发现啦~' + OriginTitle;
        titleTime = setTimeout(function() {
          document.title = OriginTitle;
        }, 2000);
      }
    });
  </script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":1},"log":false});</script></body>
</html>

<!-- 页面点击小红心 -->

      <script type="text/javascript" src="/js/clicklove.js"></script>

